[{"id":null,"title":"this is the second post\r","author":"androo\r","date":"December 31, 2022\r","content":"\r\n> ooo look a quote, something google docs doesn't support with their MD \r\n> parser. wth do they even do there.\r\n\r\n**like seriously -- what?**\r\n"},{"id":null,"title":"fixing my site and stealing christian music","author":"androo","date":"March 13, 2023","content":"Fixed my site after I accidentally broke it last month when trying to make the repo private. I made the repo public again, but I think the thing that actually broke it was that in the \"Settings\" tab of my repo the GH Pages branch was set to deploy from branch `None` instead of `gh-pages`. Might play around with trying to make this repo private again just so my coworkers don't go snooping and finding these random notes to see how much I suck at coding.\n\nOn to the other part of this title -- this sermon from TCPC Open Worship had the most BEAUTIFUL sounding performance for O Come to the Alter. The sad part is I don't even know who the singer is. Since I couldn't find a better version of it on Spotify, thought the only appropriate thing to do here was:\n-  use `pytube` to download the [YT sermon video](https://www.youtube.com/watch?v=ZM7-EwfJbpg) as an MP4\n- convert to MP3 using FFMPEG: `ffmpeg -i sermon.mp4 -vn -acodec libmp3lame -qscale:a 0 sermon.mp3` \n   - description of the fields from ChatGPT:  The -ss option specifies the start time of the crop (in this case, 30 seconds), and the -to option specifies the end time of the crop (in this case, 60 seconds). The -c:a copy option tells FFmpeg to copy the audio codec instead of re-encoding it.\n- trim the video `ffmpeg -i sermon.mp3 -ss 1:17:34 -to 1:24:06 -c:a copy come_to_the_alter.mp3`\n    - description of fields from ChatGPT: The -vn option tells FFmpeg to disable video recording, and the -acodec libmp3lame option specifies the audio codec to be used for the MP3 file. The -qscale:a option specifies the audio quality, where 0 is the best and 9 is the worst, and in this example, 2 is used.\n\nIt's a real shame TCPC OW stopped doing sermon livestreams (honest note here)... I really did like their sermons but I understand their mission to try and grow the church in person. Wish them the best and hope someday to maybe even visit them.\n"},{"id":null,"title":"Overengineering a Discord Reminder","author":"androo","date":"February 1, 2023","content":"\n## Context\nWe created a Discord server for the YF kids to use as a discussion and reminder group to read along for the book\n\"A Purpose Driven Life\" by Pastor Rick Warren. Each chapter had a title, verse to remember, point to ponder, and \nquestion. I thought it would b neat to have these all included in the daily notification. Instead of spending an\nhour to put these all in a spreadsheet to read from and automatically notify using the Integromet/Make bot\nfrom Discord, I thought it would be fun to waste ~3 hours to automate the spreadsheet population.\n\nThe data was posted on the official website for the book (albiet each chapter had its own page route) [3] and scraped\n using BeautifulSoup [4].\n\n## Workflow automation\nSetting up the workflow for going from Google Sheets -> Http API server -> Discord bot was fairly straightforard.\nUsed the references [0][1] and took a couple minutes to setup. It was somewhat straightforward. The tutorials were\nokay. It was weird how clicking on components (specifically the Spreadsheet API component) sometimes let me \nconfigure it but not configure where on the spreadsheet to start. I would have to right-click on it but it was't \nvery consistent.\n\n## Scraping\nUsed the official Purpose Driven Life website to scrape the \"point to ponder\", \"verse to remember\", and \"question\".\nWhat was awkward was the chapter titles weren't from this site directly so I couldn't scrape from there. Instead\nI just copy/pasted the table of contents in the pages from the PDF we were given and did some jank multi-line editing\nmagics with vim.\n\nInteresting, I was getting a bunch of Forbidden 403 responses when trying to scrape. Apparently the workaround\nis fairly simple: disguise the request as if it was being sent from a standard Mozilla browser. https://medium.com/@raiyanquaium/how-to-web-scrape-using-beautiful-soup-in-python-without-running-into-http-error-403-554875e5abed\n\nThere is also a few parsers out there for parsing the HTML, this answer gives a good overview of the differences https://stackoverflow.com/a/45494776\n\n## Sheeting\nUsed the Google Sheets API to dump values into a spreadsheet. The only gotcha was setting up auth. Just used the setup code from the initial \"get\" example that reads a credentials.json which generated a token.json. Auth is always\nboring/annoying imo.\n\nOverall this was a pretty fun project. I thought about creating a full Discord bot for this, but I'm lazy. And I \nwanted to use Inteogrmet/Make bot instead of hosting it myself because I actually created this while on a business\ntrip so I didn't have any computers/RPIs on hand.\n\n\n[0] https://www.make.com/en/help/app/discord#connect-to-discord-using-oauth2-and-your-own-credentials\n\n[1] https://www.integromat.com/en/help/how-to-send-a-message-to-a-discord-channel-via-http-when-a-google-sheet-is-updated\n\n[3] https://www.purposedriven.com/day1\n\n[4] https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n"}]